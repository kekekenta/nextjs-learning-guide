# ãƒãƒƒãƒå‡¦ç†ãƒ»éåŒæœŸå‡¦ç†å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ“š å­¦ç¿’ç›®æ¨™ï¼ˆ45åˆ†ï¼‰
Sidekiq/Whenever ã‹ã‚‰Next.jsã®ãƒãƒƒãƒå‡¦ç†ã¸ã®ç§»è¡Œ

## 1. ãƒãƒƒãƒå‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¯”è¼ƒ

### Railsï¼ˆSidekiq + Wheneverï¼‰
```ruby
# Gemfile
gem 'sidekiq'
gem 'sidekiq-cron'
gem 'whenever', require: false

# app/workers/data_sync_worker.rb
class DataSyncWorker
  include Sidekiq::Worker
  sidekiq_options queue: 'critical', retry: 3
  
  def perform(user_id)
    user = User.find(user_id)
    
    # å¤–éƒ¨APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    external_data = ExternalApiClient.fetch_user_data(user.external_id)
    
    # ãƒ‡ãƒ¼ã‚¿åŒæœŸ
    user.update!(
      synced_data: external_data,
      last_synced_at: Time.current
    )
    
    # å®Œäº†é€šçŸ¥
    UserMailer.sync_completed(user).deliver_later
  rescue StandardError => e
    Rails.logger.error "Sync failed for user #{user_id}: #{e.message}"
    raise # Sidekiqã«ãƒªãƒˆãƒ©ã‚¤ã•ã›ã‚‹
  end
end

# app/workers/daily_report_worker.rb
class DailyReportWorker
  include Sidekiq::Worker
  
  def perform
    Report.generate_daily_report
    AdminMailer.daily_report(report).deliver_later
  end
end

# config/schedule.rb (Whenever)
every 1.day, at: '9:00 am' do
  runner "DailyReportWorker.perform_async"
end

every 1.hour do
  runner "DataCleanupWorker.perform_async"
end

every 5.minutes do
  runner "HealthCheckWorker.perform_async"
end

# config/sidekiq.yml
:concurrency: 5
:queues:
  - critical
  - default
  - low

:schedule:
  hourly_sync:
    cron: "0 * * * *"
    class: HourlySyncWorker
  daily_cleanup:
    cron: "0 2 * * *"
    class: DailyCleanupWorker
```

### Next.js ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³

#### 1. Vercel Cron Jobs
```typescript
// app/api/cron/daily-report/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { headers } from 'next/headers';

export async function GET(request: NextRequest) {
  // Vercel Cron Jobã‹ã‚‰ã®å‘¼ã³å‡ºã—ã‚’æ¤œè¨¼
  const authHeader = headers().get('authorization');
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
  }
  
  try {
    // ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    const report = await generateDailyReport();
    
    // ãƒ¡ãƒ¼ãƒ«é€ä¿¡
    await sendEmail({
      to: process.env.ADMIN_EMAIL!,
      subject: 'Daily Report',
      body: report
    });
    
    // ãƒ­ã‚°è¨˜éŒ²
    await prisma.cronLog.create({
      data: {
        job: 'daily-report',
        status: 'success',
        executedAt: new Date()
      }
    });
    
    return NextResponse.json({ success: true });
  } catch (error) {
    console.error('Daily report failed:', error);
    
    await prisma.cronLog.create({
      data: {
        job: 'daily-report',
        status: 'failed',
        error: String(error),
        executedAt: new Date()
      }
    });
    
    return NextResponse.json({ error: 'Failed' }, { status: 500 });
  }
}

// vercel.json
{
  "crons": [
    {
      "path": "/api/cron/daily-report",
      "schedule": "0 9 * * *"
    },
    {
      "path": "/api/cron/hourly-sync",
      "schedule": "0 * * * *"
    }
  ]
}
```

#### 2. BullMQï¼ˆRedis Queueï¼‰
```typescript
// lib/queue.ts
import { Queue, Worker, QueueScheduler } from 'bullmq';
import Redis from 'ioredis';

const connection = new Redis(process.env.REDIS_URL!);

// ã‚­ãƒ¥ãƒ¼å®šç¾©
export const syncQueue = new Queue('sync', { connection });
export const reportQueue = new Queue('report', { connection });
export const emailQueue = new Queue('email', { connection });

// ã‚¸ãƒ§ãƒ–è¿½åŠ 
export async function addSyncJob(userId: string) {
  await syncQueue.add('sync-user', { userId }, {
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 2000
    },
    removeOnComplete: true,
    removeOnFail: false
  });
}

// å®šæœŸã‚¸ãƒ§ãƒ–è¨­å®š
export async function setupRecurringJobs() {
  await reportQueue.add('daily-report', {}, {
    repeat: {
      cron: '0 9 * * *',
      tz: 'Asia/Tokyo'
    }
  });
  
  await syncQueue.add('hourly-sync', {}, {
    repeat: {
      every: 60 * 60 * 1000 // 1æ™‚é–“ã”ã¨
    }
  });
}

// workers/sync-worker.ts
const syncWorker = new Worker('sync', async (job) => {
  const { userId } = job.data;
  
  console.log(`Processing sync for user ${userId}`);
  
  const user = await prisma.user.findUnique({
    where: { id: userId }
  });
  
  if (!user) {
    throw new Error(`User ${userId} not found`);
  }
  
  // å¤–éƒ¨APIå‘¼ã³å‡ºã—
  const externalData = await fetchExternalData(user.externalId);
  
  // ãƒ‡ãƒ¼ã‚¿æ›´æ–°
  await prisma.user.update({
    where: { id: userId },
    data: {
      syncedData: externalData,
      lastSyncedAt: new Date()
    }
  });
  
  // é€šçŸ¥é€ä¿¡
  await emailQueue.add('send-email', {
    to: user.email,
    subject: 'Sync Completed',
    template: 'sync-completed'
  });
  
  return { success: true, userId };
}, { connection });

// ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
syncWorker.on('failed', (job, err) => {
  console.error(`Job ${job?.id} failed:`, err);
});

syncWorker.on('completed', (job) => {
  console.log(`Job ${job.id} completed`);
});
```

#### 3. Node-cronï¼ˆã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ï¼‰
```typescript
// scripts/batch-processor.ts
import cron from 'node-cron';
import { PrismaClient } from '@prisma/client';

const prisma = new PrismaClient();

// æ¯æ—¥åˆå‰2æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
cron.schedule('0 2 * * *', async () => {
  console.log('Starting daily cleanup...');
  
  try {
    // 30æ—¥ä»¥ä¸Šå‰ã®ãƒ­ã‚°ã‚’å‰Šé™¤
    const result = await prisma.log.deleteMany({
      where: {
        createdAt: {
          lt: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000)
        }
      }
    });
    
    console.log(`Deleted ${result.count} old logs`);
    
    // æœŸé™åˆ‡ã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å‰Šé™¤
    await prisma.session.deleteMany({
      where: {
        expiresAt: {
          lt: new Date()
        }
      }
    });
  } catch (error) {
    console.error('Cleanup failed:', error);
  }
});

// 5åˆ†ã”ã¨ã«ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
cron.schedule('*/5 * * * *', async () => {
  try {
    await prisma.$queryRaw`SELECT 1`;
    console.log('Database health check: OK');
  } catch (error) {
    console.error('Database health check failed:', error);
    // ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
  }
});

// ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•
console.log('Batch processor started');

// Graceful shutdown
process.on('SIGTERM', async () => {
  console.log('Shutting down batch processor...');
  await prisma.$disconnect();
  process.exit(0);
});
```

## 2. å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†

### Railsï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
```ruby
# app/jobs/bulk_import_job.rb
class BulkImportJob < ApplicationJob
  def perform(file_path)
    CSV.foreach(file_path, headers: true).each_slice(1000) do |rows|
      users_data = rows.map do |row|
        {
          email: row['email'],
          name: row['name'],
          created_at: Time.current,
          updated_at: Time.current
        }
      end
      
      User.insert_all(users_data)
    end
  end
end

# find_in_batches
User.find_in_batches(batch_size: 1000) do |users|
  users.each do |user|
    UserProcessorWorker.perform_async(user.id)
  end
end
```

### Next.jsï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ï¼‰
```typescript
// lib/batch-import.ts
import { Readable } from 'stream';
import { pipeline } from 'stream/promises';
import { parse } from 'csv-parse';

export async function bulkImport(filePath: string) {
  const parser = parse({
    columns: true,
    skip_empty_lines: true
  });
  
  let batch: any[] = [];
  const BATCH_SIZE = 1000;
  
  parser.on('readable', async function() {
    let record;
    while ((record = parser.read()) !== null) {
      batch.push({
        email: record.email,
        name: record.name,
        createdAt: new Date(),
        updatedAt: new Date()
      });
      
      if (batch.length >= BATCH_SIZE) {
        await prisma.user.createMany({
          data: batch,
          skipDuplicates: true
        });
        batch = [];
      }
    }
  });
  
  parser.on('end', async () => {
    if (batch.length > 0) {
      await prisma.user.createMany({
        data: batch,
        skipDuplicates: true
      });
    }
  });
  
  const stream = fs.createReadStream(filePath);
  await pipeline(stream, parser);
}

// ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
async function processUsersInBatches() {
  let cursor: string | undefined;
  const batchSize = 1000;
  
  while (true) {
    const users = await prisma.user.findMany({
      take: batchSize,
      skip: cursor ? 1 : 0,
      cursor: cursor ? { id: cursor } : undefined,
      orderBy: { id: 'asc' }
    });
    
    if (users.length === 0) break;
    
    // ãƒãƒƒãƒå‡¦ç†
    await Promise.all(
      users.map(user => processUser(user))
    );
    
    cursor = users[users.length - 1].id;
  }
}
```

## 3. é•·æ™‚é–“å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã®ç®¡ç†

### Next.jsï¼ˆServer-Sent Eventsï¼‰
```typescript
// app/api/export/route.ts
export async function GET(request: NextRequest) {
  const encoder = new TextEncoder();
  
  const stream = new ReadableStream({
    async start(controller) {
      try {
        // é€²æ—çŠ¶æ³ã‚’é€ä¿¡
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({ 
            status: 'started', 
            progress: 0 
          })}\n\n`)
        );
        
        const totalCount = await prisma.user.count();
        let processed = 0;
        let cursor: string | undefined;
        
        while (true) {
          const users = await prisma.user.findMany({
            take: 100,
            skip: cursor ? 1 : 0,
            cursor: cursor ? { id: cursor } : undefined
          });
          
          if (users.length === 0) break;
          
          // ãƒ‡ãƒ¼ã‚¿å‡¦ç†
          for (const user of users) {
            await processUserExport(user);
            processed++;
            
            // é€²æ—æ›´æ–°
            if (processed % 10 === 0) {
              controller.enqueue(
                encoder.encode(`data: ${JSON.stringify({
                  status: 'processing',
                  progress: Math.round((processed / totalCount) * 100)
                })}\n\n`)
              );
            }
          }
          
          cursor = users[users.length - 1].id;
        }
        
        // å®Œäº†é€šçŸ¥
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({
            status: 'completed',
            progress: 100,
            downloadUrl: '/api/download/export.csv'
          })}\n\n`)
        );
        
        controller.close();
      } catch (error) {
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({
            status: 'error',
            error: String(error)
          })}\n\n`)
        );
        controller.close();
      }
    }
  });
  
  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  });
}

// ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´
function ExportButton() {
  const [progress, setProgress] = useState(0);
  const [status, setStatus] = useState('idle');
  
  const handleExport = () => {
    const eventSource = new EventSource('/api/export');
    
    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setStatus(data.status);
      setProgress(data.progress || 0);
      
      if (data.status === 'completed') {
        eventSource.close();
        window.location.href = data.downloadUrl;
      }
    };
    
    eventSource.onerror = () => {
      eventSource.close();
      setStatus('error');
    };
  };
  
  return (
    <div>
      <button onClick={handleExport}>Export Data</button>
      {status === 'processing' && (
        <progress value={progress} max={100}>{progress}%</progress>
      )}
    </div>
  );
}
```

## 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤

```typescript
// lib/retry.ts
export async function withRetry<T>(
  fn: () => Promise<T>,
  options: {
    maxAttempts?: number;
    delay?: number;
    backoff?: 'linear' | 'exponential';
    onRetry?: (attempt: number, error: Error) => void;
  } = {}
): Promise<T> {
  const {
    maxAttempts = 3,
    delay = 1000,
    backoff = 'exponential',
    onRetry
  } = options;
  
  let lastError: Error;
  
  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;
      
      if (attempt < maxAttempts) {
        const waitTime = backoff === 'exponential' 
          ? delay * Math.pow(2, attempt - 1)
          : delay * attempt;
        
        onRetry?.(attempt, lastError);
        await new Promise(resolve => setTimeout(resolve, waitTime));
      }
    }
  }
  
  throw lastError!;
}

// ä½¿ç”¨ä¾‹
const result = await withRetry(
  async () => {
    return await fetchExternalAPI();
  },
  {
    maxAttempts: 5,
    delay: 2000,
    backoff: 'exponential',
    onRetry: (attempt, error) => {
      console.log(`Attempt ${attempt} failed:`, error.message);
    }
  }
);
```

## 5. ç›£è¦–ã¨ãƒ­ã‚°

```typescript
// lib/batch-monitor.ts
interface BatchJob {
  id: string;
  name: string;
  status: 'pending' | 'running' | 'completed' | 'failed';
  startedAt?: Date;
  completedAt?: Date;
  error?: string;
  metadata?: any;
}

export class BatchMonitor {
  async startJob(name: string, metadata?: any): Promise<BatchJob> {
    const job = await prisma.batchJob.create({
      data: {
        name,
        status: 'running',
        startedAt: new Date(),
        metadata
      }
    });
    
    return job;
  }
  
  async completeJob(jobId: string, result?: any) {
    await prisma.batchJob.update({
      where: { id: jobId },
      data: {
        status: 'completed',
        completedAt: new Date(),
        result
      }
    });
  }
  
  async failJob(jobId: string, error: Error) {
    await prisma.batchJob.update({
      where: { id: jobId },
      data: {
        status: 'failed',
        completedAt: new Date(),
        error: error.message
      }
    });
    
    // ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
    await sendAlert({
      type: 'batch-job-failed',
      jobId,
      error: error.message
    });
  }
}

// ä½¿ç”¨ä¾‹
const monitor = new BatchMonitor();

export async function runDailyReport() {
  const job = await monitor.startJob('daily-report');
  
  try {
    const result = await generateReport();
    await monitor.completeJob(job.id, result);
  } catch (error) {
    await monitor.failJob(job.id, error as Error);
    throw error;
  }
}
```

## ğŸ¯ å®Ÿè·µæ¼”ç¿’

### èª²é¡Œ: ãƒ‡ãƒ¼ã‚¿åŒæœŸã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
1. å¤–éƒ¨APIã‹ã‚‰ã®å®šæœŸçš„ãªãƒ‡ãƒ¼ã‚¿å–å¾—
2. å·®åˆ†æ¤œå‡ºã¨æ›´æ–°
3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤
4. é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

## ğŸ’¡ é‡è¦ãªæ¯”è¼ƒ

| æ©Ÿèƒ½ | Rails | Next.js |
|------|-------|---------|
| ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ | Sidekiq | BullMQ/Vercel |
| ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ | Whenever | Cron/Vercel Cron |
| ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† | ActiveJob | Worker/API Routes |
| é•·æ™‚é–“ã‚¿ã‚¹ã‚¯ | ActionCable | SSE/WebSocket |
| ãƒãƒƒãƒå‡¦ç† | find_in_batches | Cursor pagination |
| ç›£è¦– | Sidekiq Web | ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£… |